%%%%%%%%%%%
% CONTEXT %
%%%%%%%%%%%

\section{Context}

\subsection{Softmax Output And Environmental Aspects}

The softmax output of a classifier represents the predicted probabilities for each class. In an $n$-class classification problem, the softmax function takes the logits (raw outputs) of the classifier and transforms them into a probability distribution over the $n$ classes \cite{goodfellow2016deep}. The softmax output is a vector of length $n$, where each element represents the predicted probability of the input belonging to the corresponding class. The probabilities in the softmax output sum up to 1, indicating the relative confidence of the classifier in each class prediction. The class with the highest probability is typically considered the predicted class. The softmax output provides a more informative and interpretable representation of the classifier's predictions compared to the raw logits.

When making a final decision, it is common practice to select only the class with the highest probability, effectively discarding the remaining $(n-1)/n$ of the softmax output \cite{gal2016uncertainty}. This approach, while straightforward and widely used, raises concerns about the inefficient use of the model's output and its potential environmental implications.

By considering only the class prediction, we are essentially discarding a significant portion of the information generated by the model. In a scenario with a large number of classes, this can lead to a substantial waste of computational resources and energy \cite{strubell2019energy}. The discarded softmax probabilities, which contain valuable information about the model's uncertainty and the relationships between classes, are effectively consigned to a digital landfill, contributing to the growing problem of electronic waste and energy consumption in the field of machine learning \cite{schwartz2020green}.

The environmental impact of this practice becomes more pronounced when considering the increasing scale and complexity of modern machine learning models. As the number of classes grows and models become more sophisticated, the amount of discarded information also increases, leading to a larger digital carbon footprint \cite{lacoste2019quantifying}. This is particularly concerning given the rapid growth of machine learning applications in various domains, from image and speech recognition to natural language processing and autonomous systems \cite{thompson2020computational}.

Certain approaches lend themselves well to make better use of the entire softmax output. One such approach is to incorporate uncertainty estimation techniques, such as Monte Carlo dropout \cite{gal2016dropout} or ensemble methods \cite{lakshminarayanan2017simple}, which leverage the full probability distribution to quantify the model's confidence in its predictions. By considering the uncertainty information, systems can make more informed decisions and avoid discarding potentially useful information \cite{kendall2017uncertainties}.

Another approach is to develop more efficient and environmentally-friendly machine learning techniques, such as model compression \cite{han2015deep}, quantization \cite{gholami2021survey}, and energy-aware training \cite{garcia2021estimation}. These methods aim to reduce the computational and memory requirements of models while maintaining their performance, thereby reducing the environmental impact of machine learning systems.

Furthermore, there is a growing awareness of the need for sustainable AI practices and the development of green AI frameworks \cite{schwartz2020green}. These initiatives focus on designing and implementing machine learning systems that prioritize energy efficiency, resource conservation, and environmental sustainability. By adopting sustainable AI practices, we can mitigate the environmental impact of discarded softmax outputs and other inefficiencies in machine learning pipelines.

%%%%%%%%%%%%%%%%%%%%
% SOFTMAX DISTANCE %
%%%%%%%%%%%%%%%%%%%%

% SOFTMAX DISTANCE USED AS ANOMALY DETECTION

% The concept of using the entire set of softmax prediction probabilities, rather than solely relying on the maximum output, has been extensively studied in the context of enhancing the safety, robustness, and trustworthiness of machine learning models. By considering the complete distribution of class predictions provided by the softmax output, more reliable and informative prediction pipelines may be developed, that go beyond point estimates. This approach enables the exploration of uncertainty quantification, anomaly detection, and other techniques that contribute to building safer, more robust, and trustworthy autonomous systems.
% \citet{hendrycks2018baseline} proposed a softmax prediction probability baseline for error and out-of-distribution detection across various architectures and datasets. \citet{klaus2022anomaly} uses the mean parameters of softmax function prior distribution as the cluster centroids, and unlike our approach do not use k-means to generate centroids and cluster assignments.

% \textbf{Softmax prediction probabilities}: The concept of using the entire set of softmax prediction probabilities, rather than solely relying on the maximum output, has been extensively studied in the context of enhancing the safety, robustness, and trustworthiness of machine learning models. By considering the complete distribution of class predictions provided by the softmax output, more reliable and informative prediction pipelines may be developed, that go beyond point estimates \cite{gal2016dropout}. This approach enables the exploration of uncertainty quantification, anomaly detection, and other techniques that contribute to building safer, more robust, and trustworthy autonomous systems.

% Uncertainty quantification is a crucial aspect of reliable machine learning systems, as it allows for the estimation of confidence in the model's predictions \cite{kendall2017uncertainties}. By leveraging the softmax probabilities, techniques such as Monte Carlo dropout \cite{gal2016dropout} and ensembling \cite{lakshminarayanan2017simple} can be employed to estimate the model's uncertainty. These methods help identify instances where the model is less confident, enabling the system to defer to human judgment or take a more conservative action in safety-critical scenarios \cite{michelmore2018evaluating}.

% Moreover, the softmax probabilities can be utilized for anomaly detection, which is essential for identifying out-of-distribution (OOD) samples or novel classes that the model has not encountered during training \cite{hendrycks17baseline}. By monitoring the softmax probabilities, thresholding techniques can be applied to detect anomalies based on the distribution of the predictions \cite{liang2018enhancing}. This enables the system to flag potentially problematic inputs and take appropriate actions, such as requesting human intervention or triggering fallback mechanisms.

% The use of softmax probabilities also facilitates the development of more robust models that can handle adversarial examples and other types of input perturbations \cite{goodfellow2014explaining}. Adversarial attacks aim to fool the model by crafting input samples that lead to incorrect predictions with high confidence \cite{szegedy2013intriguing}. By considering the entire softmax distribution, defensive techniques such as adversarial training \cite{madry2017towards} and input transformations \cite{guo2018countering} can be applied to improve the model's robustness against these attacks.

% Furthermore, the softmax probabilities provide valuable information for interpretability and explanability of the model's decisions \cite{ribeiro2016should}. By analyzing the distribution of the predictions, insights can be gained into the model's reasoning process and the factors that contribute to its outputs. This transparency is crucial for building trust in the system and facilitating human-machine collaboration \cite{doshi2017towards}.

% The importance of leveraging the entire softmax distribution extends to various domains, including autonomous vehicles \cite{michelmore2018evaluating}, medical diagnosis \cite{leibig2017leveraging}, and financial risk assessment \cite{feng2018deep}. In these safety-critical applications, the consequences of incorrect predictions can be severe, and relying solely on the maximum softmax output may not provide sufficient safeguards. By considering the full distribution of predictions, more informed and reliable decisions can be made, reducing the risk of catastrophic failures.

% However, the use of softmax probabilities is not without challenges. The calibration of the model's predictions is an important consideration, as poorly calibrated models may lead to overconfident or underconfident estimates \cite{guo2017calibration}. Techniques such as temperature scaling \cite{guo2017calibration} and isotonic regression \cite{zadrozny2002transforming} can be applied to improve the calibration of the softmax probabilities, ensuring that they accurately reflect the model's uncertainty.

\subsection{Using All Softmax Output Predictions}

Utilizing the entire set of softmax prediction probabilities, instead of solely depending on the maximum output, has been widely investigated to improve the safety, robustness, and trustworthiness of machine learning models. By considering the complete distribution of class predictions, more reliable and informative prediction pipelines can be developed, enabling techniques such as uncertainty quantification, anomaly detection, and robustness against adversarial attacks \cite{kendall2017uncertainties, lakshminarayanan2017simple, hendrycks17baseline, goodfellow2014explaining, szegedy2013intriguing, madry2017towards, guo2018countering}.

The softmax probabilities provide valuable information for interpretability and explainability of the model's decisions, facilitating human-machine collaboration and trust \cite{ribeiro2016should, doshi2017towards}. The importance of leveraging the entire softmax distribution extends to various safety-critical domains, such as autonomous vehicles, medical diagnosis, and financial risk assessment, where the consequences of incorrect predictions can be severe \cite{michelmore2018evaluating, leibig2017leveraging}. By considering the full distribution of predictions, more informed and reliable decisions can be made, reducing the risk of catastrophic failures.

The Misclassification Likelihood Matrix (MLMs) proposed in this study we believe has potential to further enhance the interpretability and risk mitigation capabilities of softmax probabilities. MLMs provide a comprehensive view of the model's misclassification tendencies by capturing the likelihood of each class being misclassified as another class. By analyzing the patterns and magnitudes of these misclassification likelihoods, decision-makers can identify the most common and critical sources of errors. This information can be used to prioritize model improvements, guide data collection efforts, and establish decision thresholds based on the acceptable level of risk. Moreover, MLMs can be employed to generate explanations for the model's predictions, highlighting the classes that are most likely to be confused and the factors contributing to the misclassifications. This transparency enables users to better understand the limitations and potential failure modes of the model, promoting informed decision-making and fostering trust in the system.


% The idea of utilizing the entire set of softmax prediction probabilities, instead of solely depending on the maximum output, has been widely investigated in the context of improving the safety, robustness, and trustworthiness of machine learning models. By taking into account the complete distribution of class predictions provided by the softmax output, more reliable and informative prediction pipelines can be developed, going beyond point estimates \cite{gal2016dropout}. This approach enables the exploration of uncertainty quantification, anomaly detection, and other techniques that contribute to building safer, more robust, and trustworthy autonomous systems.

% Uncertainty quantification is a vital aspect of reliable machine learning systems, as it allows for the estimation of confidence in the model's predictions \cite{kendall2017uncertainties}. By leveraging the softmax probabilities, techniques such as Monte Carlo dropout \cite{gal2016dropout} and ensembling \cite{lakshminarayanan2017simple} can be employed to estimate the model's uncertainty. These methods help identify instances where the model is less confident, enabling the system to defer to human judgment or take a more conservative action in safety-critical scenarios \cite{michelmore2018evaluating}.

% Furthermore, the softmax probabilities can be used for anomaly detection, which is crucial for identifying out-of-distribution (OOD) samples or novel classes that the model has not encountered during training \cite{hendrycks17baseline}. By monitoring the softmax probabilities, thresholding techniques can be applied to detect anomalies based on the distribution of the predictions \cite{liang2018enhancing}. This enables the system to flag potentially problematic inputs and take appropriate actions, such as requesting human intervention or triggering fallback mechanisms.

% The use of softmax probabilities also facilitates the development of more robust models that can handle adversarial examples and other types of input perturbations \cite{goodfellow2014explaining}. Adversarial attacks aim to deceive the model by crafting input samples that lead to incorrect predictions with high confidence \cite{szegedy2013intriguing}. By considering the entire softmax distribution, defensive techniques such as adversarial training \cite{madry2017towards} and input transformations \cite{guo2018countering} can be applied to improve the model's robustness against these attacks.

% Moreover, the softmax probabilities provide valuable information for interpretability and explainability of the model's decisions \cite{ribeiro2016should}. By analyzing the distribution of the predictions, insights can be gained into the model's reasoning process and the factors that contribute to its outputs. This transparency is essential for building trust in the system and facilitating human-machine collaboration \cite{doshi2017towards}.

% The importance of leveraging the entire softmax distribution extends to various domains, including autonomous vehicles \cite{michelmore2018evaluating}, medical diagnosis \cite{leibig2017leveraging}, and financial risk assessment \cite{feng2018deep}. In these safety-critical applications, the consequences of incorrect predictions can be severe, and relying solely on the maximum softmax output may not provide adequate safeguards. By considering the full distribution of predictions, more informed and reliable decisions can be made, reducing the risk of catastrophic failures.


% In a multiclass classification setting, the severity and consequences of misclassifications can vary significantly depending on the specific problem domain and the nature of the classes involved. Some misclassifications may lead to more serious repercussions than others, making it crucial to consider the relative importance of different types of errors.

% For example, in medical diagnosis, misclassifying a malignant tumor as benign can have life-threatening consequences, as it may delay necessary treatment. On the other hand, misclassifying a benign tumor as malignant may lead to unnecessary interventions and patient anxiety, but the consequences are generally less severe compared to a false negative. In this context, false negatives are considered more hazardous than false positives.

% Similarly, in autonomous driving systems, misclassifying a pedestrian as a non-pedestrian object can result in a collision and potential loss of life, while misclassifying a non-pedestrian object as a pedestrian may cause the vehicle to unnecessarily brake or take evasive action, which is less dangerous.

% Researchers have studied and addressed the varying hazards of misclassifications in different domains. \cite{kuncheva2006measures} explore the relationship between diversity measures and ensemble accuracy in classifier ensembles. They discuss the importance of considering the costs of different types of errors and how they can be incorporated into the design and evaluation of classifier ensembles.
% \cite{mozannar2020consistent} propose a framework for consistent cost-sensitive learning with nonlinear loss functions. They address the issue of varying misclassification costs and present algorithms that can effectively handle such scenarios, enabling the learning of classifiers that are sensitive to the relative hazards of different types of errors.

% By considering the complete distribution of class predictions, and quantifying the likelihood of misclassification based on this prior knowledge, techniques can be employed to develop more reliable, explainable and informative prediction pipelines. 


