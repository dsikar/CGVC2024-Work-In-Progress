%%%%%%%%%%%
% CONTEXT %
%%%%%%%%%%%

\section{Context}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Softmax Output And Environmental Aspects %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Softmax Output And Environmental Aspects}

The softmax output of a classifier represents the predicted probabilities for each class. In an $n$-class classification problem, the softmax function takes the logits (raw outputs) of the classifier and transforms them into a probability distribution over the $n$ classes \cite{goodfellow2016deep}. The softmax output is a vector of length $n$, where each element represents the predicted probability of the input belonging to the corresponding class. The probabilities in the softmax output sum up to 1, indicating the relative confidence of the classifier in each class prediction. The class with the highest probability is typically considered the predicted class. The softmax output provides a more informative and interpretable representation of the classifier's predictions compared to the raw logits.

When making a final decision, it is common practice to select only the class with the highest probability, effectively discarding the remaining $(n-1)/n$ of the softmax output \cite{gal2016uncertainty}. This approach, while straightforward and widely used, raises concerns about the inefficient use of the model's output and its potential environmental implications.

By considering only the class prediction, we are essentially discarding a significant portion of the information generated by the model. In a scenario with a large number of classes, this can lead to a substantial waste of computational resources and energy \cite{strubell2019energy}. The discarded softmax probabilities, which contain valuable information about the model's uncertainty and the relationships between classes, are effectively consigned to a digital landfill, contributing to the growing problem of electronic waste and energy consumption in the field of machine learning \cite{schwartz2020green}.

The environmental impact of this practice becomes more pronounced when considering the increasing scale and complexity of modern machine learning models. As the number of classes grows and models become more sophisticated, the amount of discarded information also increases, leading to a larger digital carbon footprint \cite{lacoste2019quantifying}. This is particularly concerning given the rapid growth of machine learning applications in various domains, from image and speech recognition to natural language processing and autonomous systems \cite{thompson2020computational}.

Certain approaches lend themselves well to make better use of the entire softmax output. One such approach is to incorporate uncertainty estimation techniques, such as Monte Carlo dropout \cite{gal2016dropout} or ensemble methods \cite{lakshminarayanan2017simple}, which leverage the full probability distribution to quantify the model's confidence in its predictions. By considering the uncertainty information, systems can make more informed decisions and avoid discarding potentially useful information \cite{kendall2017uncertainties}.

Another approach is to develop more efficient and environmentally-friendly machine learning techniques, such as model compression \cite{han2015deep}, quantization \cite{gholami2021survey}, and energy-aware training \cite{garcia2021estimation}. These methods aim to reduce the computational and memory requirements of models while maintaining their performance, thereby reducing the environmental impact of machine learning systems.

Furthermore, there is a growing awareness of the need for sustainable AI practices and the development of green AI frameworks \cite{schwartz2020green}. These initiatives focus on designing and implementing machine learning systems that prioritize energy efficiency, resource conservation, and environmental sustainability. Sustainable AI practices can help mitigate the environmental impact of discarded softmax outputs and other inefficiencies in machine learning pipelines.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Using All Softmax Output Predictions %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Using All Softmax Output Predictions}

Utilizing the entire set of softmax prediction probabilities, instead of solely depending on the maximum output, has been widely investigated to improve the safety, robustness, and trustworthiness of machine learning models. By considering the complete distribution of class predictions, more reliable and informative prediction pipelines can be developed, enabling techniques such as uncertainty quantification, anomaly detection, and robustness against adversarial attacks \cite{kendall2017uncertainties, lakshminarayanan2017simple, hendrycks17baseline, goodfellow2014explaining, szegedy2013intriguing}.

The softmax probabilities provide valuable information for interpretability and explainability of the model's decisions, facilitating human-machine collaboration and trust \cite{ribeiro2016should, doshi2017towards}. The importance of leveraging the entire softmax distribution extends to various safety-critical domains, such as autonomous vehicles, medical diagnosis, and financial risk assessment, where the consequences of incorrect predictions can be severe \cite{michelmore2018evaluating, leibig2017leveraging}. By considering the full distribution of predictions, more informed and reliable decisions can be made, reducing the risk of catastrophic failures.

The Misclassification Likelihood Matrix (MLMs) proposed in this study we believe has potential to further enhance the interpretability and risk mitigation capabilities of softmax probabilities. MLMs provide a comprehensive view of the model's misclassification tendencies by capturing the likelihood of each class being misclassified as another class. By analyzing the patterns and magnitudes of these misclassification likelihoods, decision-makers can identify the most common and critical sources of error. This information can be used to prioritize model improvements, guide data collection efforts, and establish decision thresholds based on the acceptable level of risk. Moreover, MLMs can be employed to generate explanations for the model's predictions, highlighting the classes that are most likely to be confused and the factors contributing to the misclassifications. This transparency enables users to better understand the limitations and potential failure modes of the model, promoting informed decision-making and fostering trust in the system.
