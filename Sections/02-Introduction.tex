%%%%%%%%%%%%%%%%
% INTRODUCTION %
%%%%%%%%%%%%%%%%

\section{Introduction}

% Ensuring the reliability and safety of automated decision-making systems is important, particularly related to high-impact areas where there exists potential for significant harm from errors \cite{amodei2016concrete}. Machine learning models, while powerful, are susceptible to making erroneous predictions when faced with data that differs from the distribution they were trained on \cite{hendrycks2021many}. This phenomenon, known as distribution shift, poses a significant challenge in deploying these models in real-world scenarios \cite{quinonero2009dataset}.

% Distribution shift is a pervasive issue in machine learning, occurring when the distribution of the data used to train a model differs from the distribution of the data the model encounters during deployment \cite{quinonero2009dataset}. This discrepancy can lead to a significant degradation in the model's performance, as it may struggle to generalize to the new, unseen data distribution \cite{hendrycks2019benchmarking}. Distribution shift can manifest in various forms, such as covariate shift, concept drift, and domain shift \cite{moreno2012unifying}.

% Covariate shift arises when the input data distribution changes while the conditional distribution of the output given the input remains the same \cite{shimodaira2000improving}. Concept drift, on the other hand, occurs when the relationship between the input and output variables evolves over time \cite{gama2014survey}. Domain shift refers to the situation where the model is trained on data from one domain but applied to data from a different domain \cite{patel2015visual}.

% To quantify and address distribution shift, researchers have developed various metrics and techniques. One common approach is to use statistical divergence measures, such as Kullback-Leibler (KL) divergence \cite{kullback1951information} or Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel}, to assess the difference between the training and test data distributions. These metrics provide a quantitative understanding of the extent of the distribution shift.

% Another approach is to employ domain adaptation techniques, which aim to align the feature distributions of the source and target domains \cite{wang2018deep}. This can be achieved through methods such as importance weighting \cite{sugiyama2007covariate}, feature transformation \cite{pan2009survey}, or adversarial learning \cite{ganin2016domain}. These techniques help to mitigate the impact of distribution shift by making the model more robust to changes in the data distribution.

% Recent work has also focused on developing algorithms that can detect and adapt to distribution shift in real-time \cite{lu2018learning}. These methods often rely on monitoring the model's performance on a stream of data and adjusting the model's parameters or architecture when a significant drop in performance is detected \cite{baena2006early}. Such adaptive approaches are particularly relevant in dynamic environments where the data distribution is likely to change over time.

% Therefore, distribution shift is a significant challenge in machine learning that can lead to poor model performance if not addressed properly. Researchers have proposed various metrics and techniques to quantify and mitigate the impact of distribution shift, ranging from statistical divergence measures to domain adaptation and adaptive algorithms. As machine learning models are increasingly deployed in real-world applications, developing robust methods to handle distribution shift remains an important area of research.

% In this study, we propose a novel approach for quantifying the reliability of predictions made by neural networks that may be applied under distribution shifts. Our method leverages clustering techniques to measure the distances between the outputs of a trained neural network and class centroids in the softmax distance space. By analyzing these distances, we develop a metric that provides insights into the confidence and trustworthiness of the model's predictions. Moreover, we propose a threshold below which model predictions are expected to be 100\% accurate. The proposed metric offers a practical way to determine when automated decisions can be relied upon and when human intervention is necessary, though in some time-critical scenarios, asking a human for information may not be an option \cite{amodei2016concrete}.

% Furthermore, we explore the relationship between the distance to class centroids and the model's predictive accuracy. Our findings reveal that classes predicted more accurately by the model tend to have lower softmax distances to their respective centroids, while classes predicted with lower accuracy exhibit larger distances. This observation highlights the potential of using distance-based metrics as indicators of model performance and reliability.

% The implications of our work extend beyond the realm of image classification. We are currently applying these concepts to enhance the safety of autonomous systems, specifically in the context of self-driving cars using the CARLA simulator. By identifying scenarios where human judgment is preferable to the autonomous system's assessment, our methodology aims to improve the overall safety and reliability of decision-making in complex, real-world environments.

In a multiclass classification setting, the severity and consequences of misclassifications can vary significantly depending on the specific problem domain and the nature of the classes involved. Some misclassifications may lead to more serious repercussions than others, making it crucial to consider the relative importance of different types of errors.

For example, in medical diagnosis, misclassifying a malignant tumor as benign can have life-threatening consequences, as it may delay necessary treatment. On the other hand, misclassifying a benign tumor as malignant may lead to unnecessary interventions and patient anxiety, but the consequences are generally less severe compared to a false negative. In this context, false negatives are considered more hazardous than false positives.

Similarly, in autonomous driving systems, misclassifying a pedestrian as a non-pedestrian object can result in a collision and potential loss of life, while misclassifying a non-pedestrian object as a pedestrian may cause the vehicle to unnecessarily brake or take evasive action, which is less dangerous.

Additionally, financial systems, where errors might result in significant economic losses; or legal applications, where misjudgments could affect individuals' lives.

Researchers have studied and addressed the varying hazards of misclassifications in different domains. \cite{kuncheva2006measures} explore the relationship between diversity measures and ensemble accuracy in classifier ensembles. They discuss the importance of considering the costs of different types of errors and how they can be incorporated into the design and evaluation of classifier ensembles.
\cite{mozannar2020consistent} propose a framework for consistent cost-sensitive learning with nonlinear loss functions. They address the issue of varying misclassification costs and present algorithms that can effectively handle such scenarios, enabling the learning of classifiers that are sensitive to the relative hazards of different types of errors.

Approaches such as Multi-class Difference in Confidence and Accuracy (MDCA) \cite{hebbalaguppe2022stitch}, aim to align CNN-like models' predicted confidence scores with their actual accuracy across all classes, not just the predicted class (DCA).

% Maintaining the dependability and security of automated decision-making systems is crucial, especially in high-stakes domains where mistakes can lead to severe consequences \cite{amodei2016concrete}. Although machine learning models are capable of impressive performance, they are vulnerable to making inaccurate predictions when presented with data that deviates from the distribution they were trained on \cite{hendrycks2021many}. This issue, referred to as distribution shift, presents a major obstacle in the real-world deployment of these models \cite{quinonero2009dataset}.

% Distribution shift is a widespread problem in machine learning that arises when the distribution of the training data differs from the distribution of the data encountered by the model during deployment \cite{quinonero2009dataset}. This mismatch can result in a substantial decline in the model's performance, as it may have difficulty generalizing to the new, unfamiliar data distribution \cite{hendrycks2019benchmarking}. Distribution shift can take various forms, including covariate shift, concept drift, and domain shift \cite{moreno2012unifying}.

% Covariate shift occurs when the distribution of the input data changes, but the conditional distribution of the output given the input remains constant \cite{shimodaira2000improving}. In contrast, concept drift happens when the relationship between the input and output variables changes over time \cite{gama2014survey}. Domain shift refers to the situation where a model is trained on data from one domain but is applied to data from a different domain \cite{patel2015visual}.

% To measure and tackle distribution shift, researchers have developed a range of metrics and techniques. One prevalent approach is to employ statistical divergence measures, such as Kullback-Leibler (KL) divergence \cite{kullback1951information} or Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel}, to quantify the difference between the training and test data distributions. These metrics offer a quantitative assessment of the magnitude of the distribution shift.

% Another strategy is to use domain adaptation techniques, which aim to align the feature distributions of the source and target domains \cite{wang2018deep}. This can be accomplished through methods such as importance weighting \cite{sugiyama2007covariate}, feature transformation \cite{pan2009survey}, or adversarial learning \cite{ganin2016domain}. These techniques help to mitigate the impact of distribution shift by increasing the model's robustness to changes in the data distribution.

% Recent research has also focused on developing algorithms that can detect and adapt to distribution shift in real-time \cite{lu2018learning}. These methods typically involve monitoring the model's performance on a stream of data and adjusting the model's parameters or architecture when a significant drop in performance is observed \cite{baena2006early}. Such adaptive approaches are particularly relevant in dynamic environments where the data distribution is likely to evolve over time.

Ensuring the reliability of automated decision-making systems is crucial in high-stakes domains where errors can lead to severe consequences \cite{amodei2016concrete}. Machine learning models, despite their impressive performance, are susceptible to making inaccurate predictions when presented with data that deviates from the training distribution, a problem known as distribution shift \cite{hendrycks2021many, quinonero2009dataset}.

% The reliability of automated decision-making systems is importance particularly in high-stakes domains where the consequences of errors can be severe and far-reaching \cite{amodei2016concrete}. 
% Machine learning models have demonstrated remarkable capabilities in various tasks, often matching or surpassing human performance in specific domains. However, these models are not infallible, and their impressive performance often comes with a critical caveat: they are highly sensitive to the data distribution on which they are trained. When these models encounter data that deviates significantly from their training distribution, a phenomenon known as distribution shift, their performance can degrade dramatically \cite{hendrycks2021many, quinonero2009dataset}.
% This susceptibility to distribution shift poses a significant challenge in real-world applications, where the deployment environment may differ substantially from the controlled conditions under which the model was trained. The shift can manifest in various ways: subtle changes in data collection methods, evolving trends in the problem domain, or application of the model to a slightly different context than it was originally designed for. In each case, the model may encounter input patterns that it has not been adequately prepared to handle, leading to unexpected and potentially harmful outputs.
% The impact of distribution shift is particularly concerning because it often occurs silently. Unlike explicit errors that might be easily detected, performance degradation due to distribution shift can be gradual and may go unnoticed until it leads to a critical failure.

% Distribution shift occurs when the training data distribution differs from the deployment data distribution, leading to a decline in model performance \cite{quinonero2009dataset, hendrycks2019benchmarking}. It can manifest as covariate shift, concept drift, or domain shift \cite{moreno2012unifying, shimodaira2000improving, gama2014survey, patel2015visual}. To quantify and mitigate distribution shift, researchers employ statistical divergence measures \cite{kullback1951information, gretton2012kernel}, domain adaptation techniques \cite{wang2018deep, sugiyama2007covariate, pan2009survey, ganin2016domain}, and adaptive algorithms \cite{lu2018learning, baena2006early}.



Distribution shift is a critical challenge in machine learning that occurs when the statistical properties of the data used to train a model differ from those encountered during deployment or testing \cite{quinonero2009dataset, hendrycks2019benchmarking}. This mismatch can lead to a significant decline in model performance, as the model's learned patterns may not generalize well to the new data distribution.

Distribution shift can manifest in various forms. Covariate shift occurs when the distribution of input features changes between training and deployment, while the relationship between features and labels remains the same \cite{shimodaira2000improving}. Concept drift refers to situations where the relationship between input features and the target variable changes over time, even if the input distribution remains constant \cite{gama2014survey}. Domain shift encompasses changes in both the input distribution and the relationship between inputs and outputs, often seen when applying a model to a new domain \cite{patel2015visual}. These different types of shifts present unique challenges and require tailored approaches to address them effectively.

To quantify and mitigate distribution shift, researchers have developed a range of sophisticated techniques. Statistical divergence measures, such as Kullback-Leibler divergence \cite{kullback1951information} or Maximum Mean Discrepancy \cite{gretton2012kernel}, help quantify the difference between two probability distributions, allowing researchers to detect and measure distribution shift. These measures provide valuable insights into the extent of the shift and can guide mitigation strategies.

Domain adaptation techniques aim to adapt a model trained on one domain (source) to perform well on a different but related domain (target) \cite{wang2018deep, sugiyama2007covariate, pan2009survey, ganin2016domain}. These methods include transfer learning, where knowledge gained from one task is applied to a different but related task, adversarial training, which aims to learn domain-invariant features, and importance weighting, which adjusts the contribution of training samples based on their relevance to the target domain.

Adaptive algorithms are designed to continuously update and adapt to changing data distributions \cite{lu2018learning, baena2006early}. These approaches often involve online learning or incremental learning techniques that can adjust model parameters as new data becomes available. Such algorithms are particularly useful in dynamic environments where the data distribution may evolve over time.

Quantifying the likelihood of misclassifications under distribution shift is crucial for assessing and mitigating risks in real-world deployments. By estimating the probability of incorrect predictions, decision-makers can make informed choices about when to rely on automated systems and when to defer to human judgment. This is particularly important in dynamic environments where the data distribution may evolve over time, necessitating continuous monitoring and adaptation of the models.

%Distribution shift is a major challenge in machine learning that can lead to suboptimal model performance if not addressed appropriately. Researchers have proposed various metrics and techniques to quantify and mitigate the impact of distribution shift, including statistical divergence measures, domain adaptation, and adaptive algorithms. As machine learning models are increasingly deployed in real-world applications, developing robust methods to handle distribution shift remains a critical area of research.

% In this study, we introduce a novel approach for quantifying the reliability of neural network predictions that can be applied under distribution shifts. Our method utilizes clustering techniques to measure the distances between the outputs of a trained neural network and class centroids in the softmax distance space. By analyzing these distances, we develop a metric that provides insights into the confidence and trustworthiness of the model's predictions. Furthermore, we propose a threshold below which model predictions are expected to be 100\% accurate. The proposed metric offers a practical way to determine when automated decisions can be relied upon and when human intervention is necessary, although in some time-critical scenarios, seeking human input may not be feasible \cite{amodei2016concrete}.

% Moreover, we investigate the relationship between the distance to class centroids and the model's predictive accuracy. Our findings indicate that classes predicted more accurately by the model tend to have lower softmax distances to their respective centroids, while classes predicted with lower accuracy exhibit larger distances. This observation highlights the potential of using distance-based metrics as indicators of model performance and reliability.

% The implications of our work extend beyond the realm of image classification. We are currently applying these concepts to enhance the safety of autonomous systems, specifically in the context of self-driving cars using the CARLA simulator. By identifying scenarios where human judgment is preferable to the autonomous system's assessment, our methodology aims to improve the overall safety and reliability of decision-making in complex, real-world environments.

% The main contributions from this study are:
% \begin{itemize}
% \item A novel approach for quantifying the reliability of neural network predictions by leveraging clustering techniques to measure distances between softmax outputs and class centroids.
% \item The Misclassification Likelihood Matrix.
% \end{itemize}
