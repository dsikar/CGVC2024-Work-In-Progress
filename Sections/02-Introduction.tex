%%%%%%%%%%%%%%%%
% INTRODUCTION %
%%%%%%%%%%%%%%%%

\section{Introduction}

In a multiclass classification setting, the severity and consequences of misclassifications can vary significantly depending on the specific problem domain and the nature of the classes involved. Some misclassifications may lead to more serious repercussions than others, making it crucial to consider the relative importance of different types of errors.

For example, in medical diagnosis, misclassifying a malignant tumor as benign can have life-threatening consequences, as it may delay necessary treatment. On the other hand, misclassifying a benign tumor as malignant may lead to unnecessary interventions and patient anxiety, but the consequences are generally less severe compared to a false negative. In this context, false negatives are considered more hazardous than false positives.

Similarly, in autonomous driving systems, misclassifying a pedestrian as a non-pedestrian object can result in a collision and potential loss of life, while misclassifying a non-pedestrian object as a pedestrian may cause the vehicle to unnecessarily brake or take evasive action, which is less dangerous.

Additionally, financial systems, where errors might result in significant economic losses; or legal applications, where misjudgments could affect individuals' lives.

Researchers have studied and addressed the varying hazards of misclassifications in different domains. \cite{kuncheva2006measures} explore the relationship between diversity measures and ensemble accuracy in classifier ensembles. They discuss the importance of considering the costs of different types of errors and how they can be incorporated into the design and evaluation of classifier ensembles.
\cite{mozannar2020consistent} propose a framework for consistent cost-sensitive learning with nonlinear loss functions. They address the issue of varying misclassification costs and present algorithms that can effectively handle such scenarios, enabling the learning of classifiers that are sensitive to the relative hazards of different types of errors.

Approaches such as Multi-class Difference in Confidence and Accuracy (MDCA) \cite{hebbalaguppe2022stitch}, aim to align CNN-like models' predicted confidence scores with their actual accuracy across all classes, not just the predicted class (DCA).

Ensuring the reliability of automated decision-making systems is crucial in high-stakes domains where errors can lead to severe consequences \cite{amodei2016concrete}. Machine learning models, despite their impressive performance, are susceptible to making inaccurate predictions when presented with data that deviates from the training distribution, a problem known as distribution shift \cite{hendrycks2021many, quinonero2009dataset}.

Distribution shift is a critical challenge in machine learning that occurs when the statistical properties of the data used to train a model differ from those encountered during deployment or testing \cite{quinonero2009dataset, hendrycks2019benchmarking}. This mismatch can lead to a significant decline in model performance, as the model's learned patterns may not generalize well to the new data distribution.

Distribution shift can manifest in various forms. Covariate shift occurs when the distribution of input features changes between training and deployment, while the relationship between features and labels remains the same \cite{shimodaira2000improving}. Concept drift refers to situations where the relationship between input features and the target variable changes over time, even if the input distribution remains constant \cite{gama2014survey}. Domain shift encompasses changes in both the input distribution and the relationship between inputs and outputs, often seen when applying a model to a new domain \cite{patel2015visual}. These different types of shifts present unique challenges and require tailored approaches to address them effectively.

To quantify and mitigate distribution shift, researchers have developed a range of sophisticated techniques. Statistical divergence measures, such as Kullback-Leibler divergence \cite{kullback1951information} or Maximum Mean Discrepancy \cite{gretton2012kernel}, help quantify the difference between two probability distributions, allowing researchers to detect and measure distribution shift. These measures provide valuable insights into the extent of the shift and can guide mitigation strategies.

Domain adaptation techniques aim to adapt a model trained on one domain (source) to perform well on a different but related domain (target) \cite{wang2018deep, sugiyama2007covariate, pan2009survey, ganin2016domain}. These methods include transfer learning, where knowledge gained from one task is applied to a different but related task, adversarial training, which aims to learn domain-invariant features, and importance weighting, which adjusts the contribution of training samples based on their relevance to the target domain.

Adaptive algorithms are designed to continuously update and adapt to changing data distributions \cite{lu2018learning, baena2006early}. These approaches often involve online learning or incremental learning techniques that can adjust model parameters as new data becomes available. Such algorithms are particularly useful in dynamic environments where the data distribution may evolve over time.

Quantifying the likelihood of misclassifications under distribution shift is crucial for assessing and mitigating risks in real-world deployments. By estimating the probability of incorrect predictions, decision-makers can make informed choices about when to rely on automated systems and when to defer to human judgment. This is particularly important in dynamic environments where the data distribution may evolve over time, necessitating continuous monitoring and adaptation of the models.
